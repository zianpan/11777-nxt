# AOKVQA and COCO Dataset Documentation

This repository contains the AOKVQA (A Benchmark for Visual Question Answering using World Knowledge) and COCO (Common Objects in Context) datasets. These datasets are widely used for visual question answering and object detection tasks in computer vision.

You can find detailed information about each dataset below, including instructions on how to access and use the data.

## Dataset Overview

### AOKVQA

- **Description**: AOKVQA is a visual question-answering dataset. It contains images paired with questions and answers, focusing on complex reasoning about objects and their attributes.
- **Format**: JSON files with question-answer pairs and images.
- **Size**: Approximately 25,000 question-answer pairs.

### COCO

- **Description**: COCO is a large-scale object detection, segmentation, and captioning dataset. It contains images of complex scenes with various objects annotated.
- **Format**: Images and corresponding annotations in JSON format.
- **Size**: The dataset contains 200,000 labeled images and over 1.5 million object instances.


## Accessing the Datasets

The datasets are available for download from our public S3 bucket.

### Download Links:

- **AOKVQA Repo**: [AOKVQA](https://github.com/allenai/aokvqa)
- **COCO Repo**: [COCO](https://cocodataset.org/#home)
- For COCO, I've download the image files under my S3 bucket: [Train](https://11777-aokvqa-data.s3.amazonaws.com/coco-zip/train2017.zip) [Val](https://11777-aokvqa-data.s3.amazonaws.com/coco-zip/val2017.zip) [Test](https://11777-aokvqa-data.s3.amazonaws.com/coco-zip/test2017.zip)
