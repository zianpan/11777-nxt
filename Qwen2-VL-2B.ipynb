{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32101db8-f492-4699-b55a-02cd582621c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Basic Model Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979dba8d-fdfd-4549-ac49-9dcc15d04932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef36cf76-2756-437d-a2d5-a27a0b077c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189492aae36f4d129be23c1d5209fe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab58af02fa63431080952d2dd828dc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5690b951f4c42b57fcb99220b757d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba79349b4d1422eb15758fbda5937bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06e8486937045489935cc04327f71c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2366cea61f44da09f3db3d3e29caebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c223e4254fb34fca84e7ab03345d75b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942d6f03abf54b5aabc67d8d675e9fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353e2642a6f04d03be6b47acc17ab241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988967c628d450984e157fd424598ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656f4fe8bcbe4fca8660c6ce71a99ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea38b77d7b804265a512537557ae8a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3893cc85269f4dc39e43cb8526dae50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The image depicts a serene beach scene with a woman and a dog. The woman is sitting on the sand, wearing a plaid shirt and black pants, and appears to be smiling. She is holding the dog's paw in a high-five gesture. The dog, which is a large breed, is wearing a harness and is sitting on the sand with its front paws raised. The background shows the ocean with gentle waves, and the sky is clear with a soft light, suggesting it might be either sunrise or sunset. The overall atmosphere is peaceful and joyful.\"]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "from typing import Dict\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# Load the model in half-precision on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# Image\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b7347-e27e-4960-ab97-70da7f95d4b3",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ff3aa7-c241-44b6-be97-9b18221e72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "aokvqa_dir = \"aokvqa/datasets/aokvqa\"\n",
    "coco_dir = \"aokvqa/datasets/coco\"\n",
    "\n",
    "aokvqa_dataset = json.load(open(\n",
    "        os.path.join(aokvqa_dir, f\"aokvqa_v1p0_val.json\")\n",
    "))\n",
    "\n",
    "def get_coco_path(split, image_id, coco_dir):\n",
    "    return os.path.join(coco_dir, f\"{split}2017\", f\"{image_id:012}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ccf9f7-9a01-43d6-a9b6-c5a2979028ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22jbM6gDxdaMaunuzgrsBB\n",
      "aokvqa/datasets/coco/val2017/000000461751.jpg\n",
      "What is in the motorcyclist's mouth?\n",
      "['toothpick', 'food', 'popsicle stick', 'cigarette']\n",
      "He's smoking while riding.\n"
     ]
    }
   ],
   "source": [
    "dataset_example = aokvqa_dataset[0]\n",
    "\n",
    "print(dataset_example['question_id'])\n",
    "# 22MexNkBPpdZGX6sxbxVBH\n",
    "\n",
    "image_path = get_coco_path('val', dataset_example['image_id'], coco_dir)\n",
    "print(image_path)\n",
    "# ./datasets/coco/train2017/000000299207.jpg\n",
    "\n",
    "print(dataset_example['question'])\n",
    "print(dataset_example['choices'])\n",
    "# What is the man by the bags awaiting?\n",
    "# ['skateboarder', 'train', 'delivery', 'cab']\n",
    "\n",
    "correct_choice = dataset_example['choices'][dataset_example['correct_choice_idx'] ]\n",
    "# Corrrect: cab\n",
    "\n",
    "print(dataset_example['rationales'][0])\n",
    "# A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5923f-ff7b-4f0b-abbb-a0c65910ce42",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00c9d5b-69b6-497e-9e79-3d4ec7320650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f67059f8c5497aa5e8c2838f713c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "from typing import Dict\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# Load the model in half-precision on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1238ab6b-88df-4c2a-bbbd-7cb26821b6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "logits_probs_data = []\n",
    "\n",
    "for dataset_example in aokvqa_dataset:\n",
    "    question = dataset_example['question']\n",
    "    choices = dataset_example['choices']\n",
    "    correct_choice = choices[dataset_example['correct_choice_idx']]\n",
    "    correct_idx = dataset_example['correct_choice_idx']\n",
    "    \n",
    "    image_path = get_coco_path('val', dataset_example['image_id'], coco_dir)\n",
    "    raw_image = Image.open(image_path)\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"I will give you a question and choices, return only the index of the choice\\n\"+\"Question: \"+question+\"\\nChoice: \"\\\n",
    "                 +\"0.\"+choices[0]+\" 1.\"+choices[1]+\" 2.\"+choices[2]+\" 3.\"+choices[3]+\"\\nAnswer: \"},\n",
    "                # {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        logits = output.logits\n",
    "        \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
    "    model_response = processor.decode(generated_ids[0][2:], skip_special_tokens=True)\n",
    "\n",
    "    token_probs = []\n",
    "    sequence_length = min(generated_ids[0].size(0), logits.size(1))  # Adjust the loop length to avoid out-of-bounds\n",
    "    for i, token_id in enumerate(generated_ids[0][:sequence_length]):\n",
    "        token_logit = logits[0, i]  # logits for all tokens at position i\n",
    "        token_prob = torch.nn.functional.softmax(token_logit, dim=-1)[token_id].item()\n",
    "        token_probs.append(token_prob)\n",
    "        \n",
    "    logits_probs_data.append({\n",
    "        \"question_id\": dataset_example['question_id'],\n",
    "        \"model_response\": model_response,\n",
    "        \"logits\": [logits[0, i, token_id].item() for i, token_id in enumerate(generated_ids[0][:sequence_length])],\n",
    "        \"probabilities\": token_probs\n",
    "    })\n",
    "\n",
    "    # print(repr(correct_idx))\n",
    "    # print(repr(model_response[-1]))\n",
    "    # print(str(correct_idx).strip() == str(model_response[-1]).strip())\n",
    "    if str(correct_idx).strip() == str(model_response[-1]).strip():\n",
    "        correct_count += 1\n",
    "\n",
    "print(f\"Number of accurate items: {correct_count} out of {len(aokvqa_dataset)}\")\n",
    "\n",
    "# Print logits and probabilities for analysis\n",
    "for data in logits_probs_data:\n",
    "    print(f\"Question ID: {data['question_id']}\")\n",
    "    print(f\"Model Response: {data['model_response']}\")\n",
    "    print(\"Logits:\", data[\"logits\"])\n",
    "    print(\"Probabilities:\", data[\"probabilities\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6499e21d-2705-4da4-b928-ed7d432a909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accurate items: 710 out of 1145\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of accurate items: {correct_count} out of {len(aokvqa_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649fe3d5-be2b-4e47-80b6-b888dbdbf65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of probabilities: 0.0005063655124073774\n"
     ]
    }
   ],
   "source": [
    "mean_probability = sum(data[\"probabilities\"]) / len(data[\"probabilities\"])\n",
    "print(\"Mean of probabilities:\", mean_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726a38b-6706-4897-9cf6-ccf8488faa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
