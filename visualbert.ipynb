{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangzhixian/opt/anaconda3/envs/mmml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, VisualBertForMultipleChoice\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the A-OKVQA dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/A-OKVQA\")\n",
    "\n",
    "# Extract the validation split\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Initialize CLIP model to get visual embeddings\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Preprocessing function for images\n",
    "def preprocess_image(image):\n",
    "    # Convert the image to PIL format and preprocess using CLIPProcessor\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs[\"pixel_values\"]\n",
    "\n",
    "# Function to get text embeddings using tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = VisualBertForMultipleChoice.from_pretrained(\"uclanlp/visualbert-vcr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, VisualBertForMultipleChoice\n",
    "import torch\n",
    "\n",
    "def prepare_inputs(questions, choices, tokenizer, visual_embeds):\n",
    "    \"\"\"\n",
    "    Prepare the model inputs for VisualBert with multiple choices.\n",
    "    \n",
    "    Args:\n",
    "        questions (list of str): The questions.\n",
    "        choices (list of list of str): The choices for each question.\n",
    "        tokenizer (transformers.AutoTokenizer): The tokenizer.\n",
    "        visual_embeds (torch.Tensor): The tensor of visual embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The tokenized inputs ready for the model.\n",
    "    \"\"\"\n",
    "    # Tokenize each question with its respective choices\n",
    "    inputs = tokenizer(text=questions, text_pair=choices, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    \n",
    "    # Assuming visual_embeds shape is (batch_size, num_visual_features, visual_embedding_dim)\n",
    "    visual_embeds = visual_embeds.unsqueeze(1).expand(-1, len(choices[0]), -1, -1).contiguous()\n",
    "\n",
    "    # Add visual embeddings and their corresponding masks and token type ids\n",
    "    inputs.update({\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_attention_mask\": torch.ones_like(visual_embeds[:, :, :, 0]),  # Assuming all visual inputs are attended\n",
    "        \"visual_token_type_ids\": torch.zeros_like(visual_embeds[:, :, :, 0])  # Assuming all visual tokens are of type zero\n",
    "    })\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Function to get text embeddings using tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = VisualBertForMultipleChoice.from_pretrained(\"uclanlp/visualbert-vcr\")\n",
    "\n",
    "# Example data\n",
    "questions = [\"What is in the motorcyclist's mouth?\"] * 4  # Assume batch size of 4\n",
    "choices = [[\"Gum\", \"Cigarette\", \"Nothing\", \"Candy\"]] * 4\n",
    "visual_embeds = torch.rand(4, 2048)  # Example visual features\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = prepare_inputs(questions, choices, tokenizer, visual_embeds)\n",
    "\n",
    "# Run the model\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = val_dataset[0][\"image\"]\n",
    "question = val_dataset[0][\"question\"]\n",
    "choices = val_dataset[0][\"choices\"]\n",
    "label = val_dataset[0]['choices'][val_dataset[0]['correct_choice_idx']]\n",
    "\n",
    "# Get visual embeddings\n",
    "visual_embeds = preprocess_image(image).squeeze(0)\n",
    "\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "encoding = tokenizer([[question, question], choices[2:]], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs_dict = {k: v.unsqueeze(0) for k, v in encoding.items()}\n",
    "inputs_dict.update(\n",
    "    {\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_attention_mask\": visual_attention_mask,\n",
    "        \"visual_token_type_ids\": visual_token_type_ids,\n",
    "    }\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        outputs = model(**inputs_dict)\n",
    "\n",
    "# Get the predicted answer index (use argmax)\n",
    "predicted_idx = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "# Check if the prediction matches the ground truth\n",
    "if choices[2:][predicted_idx] == label:\n",
    "    print(\"Correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'popsicle stick'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices[2:][predicted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is in the motorcyclist's mouth?\",\n",
       " \"What is in the motorcyclist's mouth?\",\n",
       " \"What is in the motorcyclist's mouth?\",\n",
       " \"What is in the motorcyclist's mouth?\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[question] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toothpick', 'food', 'popsicle stick', 'cigarette']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toothpick', 'food', 'popsicle stick', 'cigarette']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = val_dataset[0][\"choices\"]#[val_dataset[0][\"correct_choice_idx\"]]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_embeds.expand(1, 4, *visual_embeds.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x569 at 0x1B0B2DA30>\n"
     ]
    }
   ],
   "source": [
    "choices = val_dataset[0][\"image\"]\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a007db37ef564084a6ba5cbc34bd298a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edf537cfb3147239e49535d2b4dd8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53694c5def2d4b67bd53a7374f7215ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d755002dfb46af93134960da8d22fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd5863d91954ea68d9eef358f645763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a67f84be9f4661a3a1bee3c040e915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07b9e401ac64e999cb3fb4a9a6626f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d16fe4cd454e7b97d6337a1e82152d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangzhixian/opt/anaconda3/envs/mmml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Initialize CLIP model to get visual embeddings\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Preprocessing function for images\n",
    "def preprocess_image(image):\n",
    "    # Convert the image to PIL format and preprocess using CLIPProcessor\n",
    "    image_pil = Image.fromarray(image)\n",
    "    inputs = clip_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    return inputs[\"pixel_values\"]\n",
    "\n",
    "# Function to get text embeddings using tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    # Get the question text and the image\n",
    "    question = batch[\"question\"]\n",
    "    image = batch[\"image\"]\n",
    "    \n",
    "    # Get visual embeddings\n",
    "    visual_embeds = preprocess_image(image).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    # Get text tokens\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Add visual information\n",
    "    inputs.update({\n",
    "        \"visual_embeds\": visual_embeds.unsqueeze(0),  # Add batch dimension back\n",
    "        \"visual_token_type_ids\": torch.ones(visual_embeds.shape[:-1], dtype=torch.long).unsqueeze(0),\n",
    "        \"visual_attention_mask\": torch.ones(visual_embeds.shape[:-1], dtype=torch.float).unsqueeze(0),\n",
    "    })\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader with batch size 1 (to match the expected input size of VisualBERT)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, collate_fn=prepare_batch)\n",
    "\n",
    "# Load VisualBERT model for VQA\n",
    "model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through the validation set\n",
    "for batch in val_dataloader:\n",
    "    # Forward pass through VisualBERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    # Get the predicted answer index (use argmax)\n",
    "    predicted_idx = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    \n",
    "    # Check if the prediction matches the ground truth\n",
    "    if predicted_idx == batch[\"correct_choice_idx\"]:\n",
    "        correct += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'question_id', 'question', 'choices', 'correct_choice_idx', 'direct_answers', 'difficult_direct_answer', 'rationales'],\n",
      "        num_rows: 17056\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'question_id', 'question', 'choices', 'correct_choice_idx', 'direct_answers', 'difficult_direct_answer', 'rationales'],\n",
      "        num_rows: 1145\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'question_id', 'question', 'choices', 'correct_choice_idx', 'direct_answers', 'difficult_direct_answer', 'rationales'],\n",
      "        num_rows: 6702\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(mode=None, decode=True, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, VisualBertForQuestionAnswering\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ExifTags\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A-OKVQA dataset (only validation set)\n",
    "dataset = load_dataset(\"HuggingFaceM4/A-OKVQA\", split=\"validation[:200]\")  # Limit to 200 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
